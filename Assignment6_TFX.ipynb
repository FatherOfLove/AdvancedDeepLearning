{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment6_TFX.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOsiECAef5bTjqIWp4Y1eBK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatherOfLove/AdvancedDeepLearning/blob/master/Assignment6_TFX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67dneN7arzV_"
      },
      "source": [
        "try:\r\n",
        "  import colab\r\n",
        "  !pip install --upgrade pip\r\n",
        "except:\r\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lagU0431sG_r"
      },
      "source": [
        "pip install -q -U --use-feature=2020-resolver tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOUK9VIIseOu"
      },
      "source": [
        "import package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poEatw_7r_P0"
      },
      "source": [
        "import os\r\n",
        "import pprint\r\n",
        "import tempfile\r\n",
        "import urllib\r\n",
        "\r\n",
        "import absl\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_model_analysis as tfma\r\n",
        "tf.get_logger().propagate = False\r\n",
        "pp = pprint.PrettyPrinter()\r\n",
        "\r\n",
        "import tfx\r\n",
        "from tfx.components import CsvExampleGen\r\n",
        "from tfx.components import Evaluator\r\n",
        "from tfx.components import ExampleValidator\r\n",
        "from tfx.components import Pusher\r\n",
        "from tfx.components import ResolverNode\r\n",
        "from tfx.components import SchemaGen\r\n",
        "from tfx.components import StatisticsGen\r\n",
        "from tfx.components import Trainer\r\n",
        "from tfx.components import Transform\r\n",
        "from tfx.components.base import executor_spec\r\n",
        "from tfx.components.trainer.executor import GenericExecutor\r\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\r\n",
        "from tfx.orchestration import metadata\r\n",
        "from tfx.orchestration import pipeline\r\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\r\n",
        "from tfx.proto import pusher_pb2\r\n",
        "from tfx.proto import trainer_pb2\r\n",
        "from tfx.types import Channel\r\n",
        "from tfx.types.standard_artifacts import Model\r\n",
        "from tfx.types.standard_artifacts import ModelBlessing\r\n",
        "from tfx.utils.dsl_utils import external_input\r\n",
        "\r\n",
        "\r\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBbjy3iCsiJy"
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\r\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei_oJcE1s0_6"
      },
      "source": [
        "# This is the root directory for your TFX pip package installation.\r\n",
        "_tfx_root = tfx.__path__[0]\r\n",
        "\r\n",
        "# This is the directory containing the TFX Chicago Taxi Pipeline example.\r\n",
        "_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\r\n",
        "\r\n",
        "# This is the path where your model will be pushed for serving.\r\n",
        "_serving_model_dir = os.path.join(\r\n",
        "    tempfile.mkdtemp(), 'serving_model/taxi_simple')\r\n",
        "\r\n",
        "# Set up logging.\r\n",
        "absl.logging.set_verbosity(absl.logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whmwcM2ks5Gd"
      },
      "source": [
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\r\n",
        "DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\r\n",
        "_data_filepath = os.path.join(_data_root, \"data.csv\")\r\n",
        "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex9HSDFEs_Jx"
      },
      "source": [
        "context = InteractiveContext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJekKa5OtSIY"
      },
      "source": [
        "example_gen = CsvExampleGen(input=external_input(_data_root))\r\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V2OIhDJtbb7"
      },
      "source": [
        "artifact = example_gen.outputs['examples'].get()[0]\r\n",
        "print(artifact.split_names, artifact.uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgCewDcQtfCN"
      },
      "source": [
        "# Get the URI of the output artifact representing the training examples, which is a directory\r\n",
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\r\n",
        "\r\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\r\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\r\n",
        "                      for name in os.listdir(train_uri)]\r\n",
        "\r\n",
        "# Create a `TFRecordDataset` to read these files\r\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\r\n",
        "\r\n",
        "# Iterate over the first 3 records and decode them.\r\n",
        "for tfrecord in dataset.take(3):\r\n",
        "  serialized_example = tfrecord.numpy()\r\n",
        "  example = tf.train.Example()\r\n",
        "  example.ParseFromString(serialized_example)\r\n",
        "  pp.pprint(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6KcFi49tg47"
      },
      "source": [
        "statistics_gen = StatisticsGen(\r\n",
        "    examples=example_gen.outputs['examples'])\r\n",
        "context.run(statistics_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gevqPvVztjdI"
      },
      "source": [
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn6oEbR-tmxV"
      },
      "source": [
        "schema_gen = SchemaGen(\r\n",
        "    statistics=statistics_gen.outputs['statistics'],\r\n",
        "    infer_feature_shape=False)\r\n",
        "context.run(schema_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm9OglCNtq1w"
      },
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1shaTBVtscx"
      },
      "source": [
        "example_validator = ExampleValidator(\r\n",
        "    statistics=statistics_gen.outputs['statistics'],\r\n",
        "    schema=schema_gen.outputs['schema'])\r\n",
        "context.run(example_validator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXGQ8luBtu37"
      },
      "source": [
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEcpLBwAtwcC"
      },
      "source": [
        "_taxi_constants_module_file = 'taxi_constants.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tgyWFUYtyWO"
      },
      "source": [
        "%%writefile {_taxi_constants_module_file}\r\n",
        "\r\n",
        "# Categorical features are assumed to each have a maximum value in the dataset.\r\n",
        "MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\r\n",
        "\r\n",
        "CATEGORICAL_FEATURE_KEYS = [\r\n",
        "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\r\n",
        "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\r\n",
        "    'dropoff_community_area'\r\n",
        "]\r\n",
        "\r\n",
        "DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\r\n",
        "\r\n",
        "# Number of buckets used by tf.transform for encoding each feature.\r\n",
        "FEATURE_BUCKET_COUNT = 10\r\n",
        "\r\n",
        "BUCKET_FEATURE_KEYS = [\r\n",
        "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\r\n",
        "    'dropoff_longitude'\r\n",
        "]\r\n",
        "\r\n",
        "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\r\n",
        "VOCAB_SIZE = 1000\r\n",
        "\r\n",
        "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\r\n",
        "OOV_SIZE = 10\r\n",
        "\r\n",
        "VOCAB_FEATURE_KEYS = [\r\n",
        "    'payment_type',\r\n",
        "    'company',\r\n",
        "]\r\n",
        "\r\n",
        "# Keys\r\n",
        "LABEL_KEY = 'tips'\r\n",
        "FARE_KEY = 'fare'\r\n",
        "\r\n",
        "def transformed_name(key):\r\n",
        "  return key + '_xf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVYUx41Tt0U4"
      },
      "source": [
        "_taxi_transform_module_file = 'taxi_transform.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYpEI3qut2FP"
      },
      "source": [
        "%%writefile {_taxi_transform_module_file}\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_transform as tft\r\n",
        "\r\n",
        "import taxi_constants\r\n",
        "\r\n",
        "_DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS\r\n",
        "_VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS\r\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\r\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\r\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\r\n",
        "_BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS\r\n",
        "_CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS\r\n",
        "_FARE_KEY = taxi_constants.FARE_KEY\r\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\r\n",
        "_transformed_name = taxi_constants.transformed_name\r\n",
        "\r\n",
        "\r\n",
        "def preprocessing_fn(inputs):\r\n",
        "  \"\"\"tf.transform's callback function for preprocessing inputs.\r\n",
        "  Args:\r\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\r\n",
        "  Returns:\r\n",
        "    Map from string feature key to transformed feature operations.\r\n",
        "  \"\"\"\r\n",
        "  outputs = {}\r\n",
        "  for key in _DENSE_FLOAT_FEATURE_KEYS:\r\n",
        "    # Preserve this feature as a dense float, setting nan's to the mean.\r\n",
        "    outputs[_transformed_name(key)] = tft.scale_to_z_score(\r\n",
        "        _fill_in_missing(inputs[key]))\r\n",
        "\r\n",
        "  for key in _VOCAB_FEATURE_KEYS:\r\n",
        "    # Build a vocabulary for this feature.\r\n",
        "    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\r\n",
        "        _fill_in_missing(inputs[key]),\r\n",
        "        top_k=_VOCAB_SIZE,\r\n",
        "        num_oov_buckets=_OOV_SIZE)\r\n",
        "\r\n",
        "  for key in _BUCKET_FEATURE_KEYS:\r\n",
        "    outputs[_transformed_name(key)] = tft.bucketize(\r\n",
        "        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)\r\n",
        "\r\n",
        "  for key in _CATEGORICAL_FEATURE_KEYS:\r\n",
        "    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])\r\n",
        "\r\n",
        "  # Was this passenger a big tipper?\r\n",
        "  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\r\n",
        "  tips = _fill_in_missing(inputs[_LABEL_KEY])\r\n",
        "  outputs[_transformed_name(_LABEL_KEY)] = tf.where(\r\n",
        "      tf.math.is_nan(taxi_fare),\r\n",
        "      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\r\n",
        "      # Test if the tip was > 20% of the fare.\r\n",
        "      tf.cast(\r\n",
        "          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\r\n",
        "\r\n",
        "  return outputs\r\n",
        "\r\n",
        "\r\n",
        "def _fill_in_missing(x):\r\n",
        "  \"\"\"Replace missing values in a SparseTensor.\r\n",
        "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\r\n",
        "  Args:\r\n",
        "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\r\n",
        "      in the second dimension.\r\n",
        "  Returns:\r\n",
        "    A rank 1 tensor where missing values of `x` have been filled in.\r\n",
        "  \"\"\"\r\n",
        "  default_value = '' if x.dtype == tf.string else 0\r\n",
        "  return tf.squeeze(\r\n",
        "      tf.sparse.to_dense(\r\n",
        "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\r\n",
        "          default_value),\r\n",
        "      axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7-IV_zJt3f3"
      },
      "source": [
        "transform = Transform(\r\n",
        "    examples=example_gen.outputs['examples'],\r\n",
        "    schema=schema_gen.outputs['schema'],\r\n",
        "    module_file=os.path.abspath(_taxi_transform_module_file))\r\n",
        "context.run(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gWexiqGt420"
      },
      "source": [
        "transform.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oWkuVhbt7XX"
      },
      "source": [
        "train_uri = transform.outputs['transform_graph'].get()[0].uri\r\n",
        "os.listdir(train_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnO0DPpWt83j"
      },
      "source": [
        "# Get the URI of the output artifact representing the transformed examples, which is a directory\r\n",
        "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'train')\r\n",
        "\r\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\r\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\r\n",
        "                      for name in os.listdir(train_uri)]\r\n",
        "\r\n",
        "# Create a `TFRecordDataset` to read these files\r\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\r\n",
        "\r\n",
        "# Iterate over the first 3 records and decode them.\r\n",
        "for tfrecord in dataset.take(3):\r\n",
        "  serialized_example = tfrecord.numpy()\r\n",
        "  example = tf.train.Example()\r\n",
        "  example.ParseFromString(serialized_example)\r\n",
        "  pp.pprint(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2hmXX5Qt-LG"
      },
      "source": [
        "_taxi_trainer_module_file = 'taxi_trainer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfu_ij7Ut_-o"
      },
      "source": [
        "%%writefile {_taxi_trainer_module_file}\r\n",
        "\r\n",
        "from typing import List, Text\r\n",
        "\r\n",
        "import os\r\n",
        "import absl\r\n",
        "import datetime\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_transform as tft\r\n",
        "\r\n",
        "from tfx.components.trainer.executor import TrainerFnArgs\r\n",
        "from tfx.components.trainer.fn_args_utils import DataAccessor\r\n",
        "from tfx_bsl.tfxio import dataset_options\r\n",
        "\r\n",
        "import taxi_constants\r\n",
        "\r\n",
        "_DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS\r\n",
        "_VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS\r\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\r\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\r\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\r\n",
        "_BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS\r\n",
        "_CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS\r\n",
        "_MAX_CATEGORICAL_FEATURE_VALUES = taxi_constants.MAX_CATEGORICAL_FEATURE_VALUES\r\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\r\n",
        "_transformed_name = taxi_constants.transformed_name\r\n",
        "\r\n",
        "\r\n",
        "def _transformed_names(keys):\r\n",
        "  return [_transformed_name(key) for key in keys]\r\n",
        "\r\n",
        "\r\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\r\n",
        "  \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\r\n",
        "\r\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\r\n",
        "\r\n",
        "  @tf.function\r\n",
        "  def serve_tf_examples_fn(serialized_tf_examples):\r\n",
        "    \"\"\"Returns the output to be used in the serving signature.\"\"\"\r\n",
        "    feature_spec = tf_transform_output.raw_feature_spec()\r\n",
        "    feature_spec.pop(_LABEL_KEY)\r\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\r\n",
        "    transformed_features = model.tft_layer(parsed_features)\r\n",
        "    return model(transformed_features)\r\n",
        "\r\n",
        "  return serve_tf_examples_fn\r\n",
        "\r\n",
        "\r\n",
        "def _input_fn(file_pattern: List[Text],\r\n",
        "              data_accessor: DataAccessor,\r\n",
        "              tf_transform_output: tft.TFTransformOutput,\r\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\r\n",
        "  \"\"\"Generates features and label for tuning/training.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\r\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\r\n",
        "    tf_transform_output: A TFTransformOutput.\r\n",
        "    batch_size: representing the number of consecutive elements of returned\r\n",
        "      dataset to combine in a single batch\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    A dataset that contains (features, indices) tuple where features is a\r\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\r\n",
        "  \"\"\"\r\n",
        "  return data_accessor.tf_dataset_factory(\r\n",
        "      file_pattern,\r\n",
        "      dataset_options.TensorFlowDatasetOptions(\r\n",
        "          batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),\r\n",
        "      tf_transform_output.transformed_metadata.schema)\r\n",
        "\r\n",
        "\r\n",
        "def _build_keras_model(hidden_units: List[int] = None) -> tf.keras.Model:\r\n",
        "  \"\"\"Creates a DNN Keras model for classifying taxi data.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    hidden_units: [int], the layer sizes of the DNN (input layer first).\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    A keras Model.\r\n",
        "  \"\"\"\r\n",
        "  real_valued_columns = [\r\n",
        "      tf.feature_column.numeric_column(key, shape=())\r\n",
        "      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\r\n",
        "  ]\r\n",
        "  categorical_columns = [\r\n",
        "      tf.feature_column.categorical_column_with_identity(\r\n",
        "          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\r\n",
        "      for key in _transformed_names(_VOCAB_FEATURE_KEYS)\r\n",
        "  ]\r\n",
        "  categorical_columns += [\r\n",
        "      tf.feature_column.categorical_column_with_identity(\r\n",
        "          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\r\n",
        "      for key in _transformed_names(_BUCKET_FEATURE_KEYS)\r\n",
        "  ]\r\n",
        "  categorical_columns += [\r\n",
        "      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension\r\n",
        "          key,\r\n",
        "          num_buckets=num_buckets,\r\n",
        "          default_value=0) for key, num_buckets in zip(\r\n",
        "              _transformed_names(_CATEGORICAL_FEATURE_KEYS),\r\n",
        "              _MAX_CATEGORICAL_FEATURE_VALUES)\r\n",
        "  ]\r\n",
        "  indicator_column = [\r\n",
        "      tf.feature_column.indicator_column(categorical_column)\r\n",
        "      for categorical_column in categorical_columns\r\n",
        "  ]\r\n",
        "\r\n",
        "  model = _wide_and_deep_classifier(\r\n",
        "      # TODO(b/139668410) replace with premade wide_and_deep keras model\r\n",
        "      wide_columns=indicator_column,\r\n",
        "      deep_columns=real_valued_columns,\r\n",
        "      dnn_hidden_units=hidden_units or [100, 70, 50, 25])\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "def _wide_and_deep_classifier(wide_columns, deep_columns, dnn_hidden_units):\r\n",
        "  \"\"\"Build a simple keras wide and deep model.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    wide_columns: Feature columns wrapped in indicator_column for wide (linear)\r\n",
        "      part of the model.\r\n",
        "    deep_columns: Feature columns for deep part of the model.\r\n",
        "    dnn_hidden_units: [int], the layer sizes of the hidden DNN.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    A Wide and Deep Keras model\r\n",
        "  \"\"\"\r\n",
        "  # Following values are hard coded for simplicity in this example,\r\n",
        "  # However prefarably they should be passsed in as hparams.\r\n",
        "\r\n",
        "  # Keras needs the feature definitions at compile time.\r\n",
        "  # TODO(b/139081439): Automate generation of input layers from FeatureColumn.\r\n",
        "  input_layers = {\r\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype=tf.float32)\r\n",
        "      for colname in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\r\n",
        "  }\r\n",
        "  input_layers.update({\r\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\r\n",
        "      for colname in _transformed_names(_VOCAB_FEATURE_KEYS)\r\n",
        "  })\r\n",
        "  input_layers.update({\r\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\r\n",
        "      for colname in _transformed_names(_BUCKET_FEATURE_KEYS)\r\n",
        "  })\r\n",
        "  input_layers.update({\r\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\r\n",
        "      for colname in _transformed_names(_CATEGORICAL_FEATURE_KEYS)\r\n",
        "  })\r\n",
        "\r\n",
        "  # TODO(b/161952382): Replace with Keras preprocessing layers.\r\n",
        "  deep = tf.keras.layers.DenseFeatures(deep_columns)(input_layers)\r\n",
        "  for numnodes in dnn_hidden_units:\r\n",
        "    deep = tf.keras.layers.Dense(numnodes)(deep)\r\n",
        "  wide = tf.keras.layers.DenseFeatures(wide_columns)(input_layers)\r\n",
        "\r\n",
        "  output = tf.keras.layers.Dense(\r\n",
        "      1, activation='sigmoid')(\r\n",
        "          tf.keras.layers.concatenate([deep, wide]))\r\n",
        "\r\n",
        "  model = tf.keras.Model(input_layers, output)\r\n",
        "  model.compile(\r\n",
        "      loss='binary_crossentropy',\r\n",
        "      optimizer=tf.keras.optimizers.Adam(lr=0.001),\r\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy()])\r\n",
        "  model.summary(print_fn=absl.logging.info)\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "# TFX Trainer will call this function.\r\n",
        "def run_fn(fn_args: TrainerFnArgs):\r\n",
        "  \"\"\"Train the model based on given args.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\r\n",
        "  \"\"\"\r\n",
        "  # Number of nodes in the first layer of the DNN\r\n",
        "  first_dnn_layer_size = 100\r\n",
        "  num_dnn_layers = 4\r\n",
        "  dnn_decay_factor = 0.7\r\n",
        "\r\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\r\n",
        "\r\n",
        "  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, \r\n",
        "                            tf_transform_output, 40)\r\n",
        "  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, \r\n",
        "                           tf_transform_output, 40)\r\n",
        "\r\n",
        "  model = _build_keras_model(\r\n",
        "      # Construct layers sizes with exponetial decay\r\n",
        "      hidden_units=[\r\n",
        "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\r\n",
        "          for i in range(num_dnn_layers)\r\n",
        "      ])\r\n",
        "\r\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\r\n",
        "      log_dir=fn_args.model_run_dir, update_freq='batch')\r\n",
        "  model.fit(\r\n",
        "      train_dataset,\r\n",
        "      steps_per_epoch=fn_args.train_steps,\r\n",
        "      validation_data=eval_dataset,\r\n",
        "      validation_steps=fn_args.eval_steps,\r\n",
        "      callbacks=[tensorboard_callback])\r\n",
        "\r\n",
        "  signatures = {\r\n",
        "      'serving_default':\r\n",
        "          _get_serve_tf_examples_fn(model,\r\n",
        "                                    tf_transform_output).get_concrete_function(\r\n",
        "                                        tf.TensorSpec(\r\n",
        "                                            shape=[None],\r\n",
        "                                            dtype=tf.string,\r\n",
        "                                            name='examples')),\r\n",
        "  }\r\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iLRDn1KuBKJ"
      },
      "source": [
        "trainer = Trainer(\r\n",
        "    module_file=os.path.abspath(_taxi_trainer_module_file),\r\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\r\n",
        "    examples=transform.outputs['transformed_examples'],\r\n",
        "    transform_graph=transform.outputs['transform_graph'],\r\n",
        "    schema=schema_gen.outputs['schema'],\r\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\r\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5))\r\n",
        "context.run(trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDTeGqJguDA5"
      },
      "source": [
        "model_artifact_dir = trainer.outputs['model'].get()[0].uri\r\n",
        "pp.pprint(os.listdir(model_artifact_dir))\r\n",
        "model_dir = os.path.join(model_artifact_dir, 'serving_model_dir')\r\n",
        "pp.pprint(os.listdir(model_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV5VOxkfvNEd"
      },
      "source": [
        "model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\r\n",
        "\r\n",
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir {model_run_artifact_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSZ7Ewp8vNSX"
      },
      "source": [
        "eval_config = tfma.EvalConfig(\r\n",
        "    model_specs=[\r\n",
        "        # This assumes a serving model with signature 'serving_default'. If\r\n",
        "        # using estimator based EvalSavedModel, add signature_name: 'eval' and \r\n",
        "        # remove the label_key.\r\n",
        "        tfma.ModelSpec(label_key='tips')\r\n",
        "    ],\r\n",
        "    metrics_specs=[\r\n",
        "        tfma.MetricsSpec(\r\n",
        "            # The metrics added here are in addition to those saved with the\r\n",
        "            # model (assuming either a keras model or EvalSavedModel is used).\r\n",
        "            # Any metrics added into the saved model (for example using\r\n",
        "            # model.compile(..., metrics=[...]), etc) will be computed\r\n",
        "            # automatically.\r\n",
        "            # To add validation thresholds for metrics saved with the model,\r\n",
        "            # add them keyed by metric name to the thresholds map.\r\n",
        "            metrics=[\r\n",
        "                tfma.MetricConfig(class_name='ExampleCount'),\r\n",
        "                tfma.MetricConfig(class_name='BinaryAccuracy',\r\n",
        "                  threshold=tfma.MetricThreshold(\r\n",
        "                      value_threshold=tfma.GenericValueThreshold(\r\n",
        "                          lower_bound={'value': 0.5}),\r\n",
        "                      change_threshold=tfma.GenericChangeThreshold(\r\n",
        "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\r\n",
        "                          absolute={'value': -1e-10})))\r\n",
        "            ]\r\n",
        "        )\r\n",
        "    ],\r\n",
        "    slicing_specs=[\r\n",
        "        # An empty slice spec means the overall slice, i.e. the whole dataset.\r\n",
        "        tfma.SlicingSpec(),\r\n",
        "        # Data can be sliced along a feature column. In this case, data is\r\n",
        "        # sliced along feature column trip_start_hour.\r\n",
        "        tfma.SlicingSpec(feature_keys=['trip_start_hour'])\r\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD1n4re0vOcW"
      },
      "source": [
        "# Use TFMA to compute a evaluation statistics over features of a model and\r\n",
        "# validate them against a baseline.\r\n",
        "\r\n",
        "# The model resolver is only required if performing model validation in addition\r\n",
        "# to evaluation. In this case we validate against the latest blessed model. If\r\n",
        "# no model has been blessed before (as in this case) the evaluator will make our\r\n",
        "# candidate the first blessed model.\r\n",
        "model_resolver = ResolverNode(\r\n",
        "      instance_name='latest_blessed_model_resolver',\r\n",
        "      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\r\n",
        "      model=Channel(type=Model),\r\n",
        "      model_blessing=Channel(type=ModelBlessing))\r\n",
        "context.run(model_resolver)\r\n",
        "\r\n",
        "evaluator = Evaluator(\r\n",
        "    examples=example_gen.outputs['examples'],\r\n",
        "    model=trainer.outputs['model'],\r\n",
        "    baseline_model=model_resolver.outputs['model'],\r\n",
        "    # Change threshold will be ignored if there is no baseline (first run).\r\n",
        "    eval_config=eval_config)\r\n",
        "context.run(evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q95edK19vP1y"
      },
      "source": [
        "evaluator.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HLGdGHUvRYt"
      },
      "source": [
        "context.show(evaluator.outputs['evaluation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jztC1wwXvTGl"
      },
      "source": [
        "import tensorflow_model_analysis as tfma\r\n",
        "\r\n",
        "# Get the TFMA output result path and load the result.\r\n",
        "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\r\n",
        "tfma_result = tfma.load_eval_result(PATH_TO_RESULT)\r\n",
        "\r\n",
        "# Show data sliced along feature column trip_start_hour.\r\n",
        "tfma.view.render_slicing_metrics(\r\n",
        "    tfma_result, slicing_column='trip_start_hour')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJTJUoB-vUPR"
      },
      "source": [
        "blessing_uri = evaluator.outputs.blessing.get()[0].uri\r\n",
        "!ls -l {blessing_uri}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGAVZB_EvVU9"
      },
      "source": [
        "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\r\n",
        "print(tfma.load_validation_result(PATH_TO_RESULT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8juljqRmvXGF"
      },
      "source": [
        "pusher = Pusher(\r\n",
        "    model=trainer.outputs['model'],\r\n",
        "    model_blessing=evaluator.outputs['blessing'],\r\n",
        "    push_destination=pusher_pb2.PushDestination(\r\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\r\n",
        "            base_directory=_serving_model_dir)))\r\n",
        "context.run(pusher)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Y-Kdg4vZI_"
      },
      "source": [
        "pusher.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQCPSHUXvabu"
      },
      "source": [
        "push_uri = pusher.outputs.model_push.get()[0].uri\r\n",
        "model = tf.saved_model.load(push_uri)\r\n",
        "\r\n",
        "for item in model.signatures.items():\r\n",
        "  pp.pprint(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7gHzeIJvbkI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}